{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Process API data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingreedypy import Ingreedy\n",
    "# Ingreedy is a recipe parser downloaded from GitHub\n",
    "import pandas as pd\n",
    "\n",
    "# Open json file with API data\n",
    "import json\n",
    "with open('allrecipes.json', 'r') as f:\n",
    "     data = json.load(f)\n",
    "\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop everything that is not from allrecipes \n",
    "\n",
    "allrecipes = []\n",
    "for i in range (1,len(data)):\n",
    "    if data[i]['recipe']['publisher']=='All Recipes':\n",
    "        allrecipes.append(data[i])\n",
    "data=allrecipes\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate recipes\n",
    "\n",
    "data=[i for n, i in enumerate(data) if i not in data[n + 1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lookup lists for sugar, flour and fat, parse each recipe/ingredient, assign amts to new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sugar_lkp=['sugar','honey','maple syrup']\n",
    "fat_lkp=[' oil','butter','margarine','shortening']\n",
    "flour_lkp=['flour','oatmeal','meal','cornmeal','oats']\n",
    "\n",
    "# Conversion dictionary for recipe units\n",
    "convert=dict([(\"cup\",1),(\"fluid_ounce\",0.13),(\"gallon\",16),(\"ounce\",0.13),(\"pint\",2),(\"pound\",2.25),(\"quart\",4),\n",
    "              (\"tablespoon\",0.06),(\"teaspoon\",0.02),(\"gram\",0.01),(\"kilogram\",8),(\"liter\",4.23)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lkp(string1,list2):\n",
    "    for word in list2:\n",
    "        if word in string1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sugar_fat(ingredients):\n",
    "    sugar=[]\n",
    "    fat=[]\n",
    "    flour=[]\n",
    "    amt_sugar=0\n",
    "    amt_fat=0\n",
    "    amt_flour=0\n",
    "    #ingredients=data[i]['recipe']['ingredients']\n",
    "    for item in ingredients:\n",
    "        item=item.replace(',','')\n",
    "        item=item.replace(')','')\n",
    "        item=item.replace('(','')\n",
    "        item=item.replace('-',' ')\n",
    "        item=item.replace('&frac12','1/2')\n",
    "        item=item.replace('&frac14','1/4')\n",
    "        item=item.replace('&frac34','3/4')\n",
    "        item=item.replace(';','')\n",
    "        if lkp(item.lower(),sugar_lkp):\n",
    "            try:\n",
    "                p= Ingreedy().parse(item)\n",
    "                p['amount']=convert[p['unit']]*p['amount']\n",
    "                sugar.append(p)\n",
    "            except:\n",
    "                pass\n",
    "        if lkp(item.lower(),fat_lkp):\n",
    "            try:\n",
    "                p= Ingreedy().parse(item)\n",
    "                p['amount']=convert[p['unit']]*p['amount']\n",
    "                fat.append(p)\n",
    "            except:\n",
    "                pass\n",
    "        if lkp(item.lower(),flour_lkp):\n",
    "            try:\n",
    "                p= Ingreedy().parse(item)\n",
    "                p['amount']=convert[p['unit']]*p['amount']\n",
    "                flour.append(p)\n",
    "            except:\n",
    "                pass\n",
    "    for i in range(0,len(sugar)):\n",
    "        try:\n",
    "            amt_sugar=amt_sugar+sugar[i]['amount']\n",
    "        except:\n",
    "            pass\n",
    "    for i in range(0,len(fat)):\n",
    "        try:\n",
    "            amt_fat=amt_fat+fat[i]['amount']\n",
    "        except:\n",
    "            pass\n",
    "    for i in range(0,len(flour)):\n",
    "        try:\n",
    "            amt_flour=amt_flour+flour[i]['amount']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return amt_sugar, amt_fat, amt_flour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through each recipe and calculate amounts of fat, flour, sugar. Assign amts to new columns\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    recipe = data[i]['recipe']['ingredients']\n",
    "    amt_sugar, amt_fat, amt_flour = sugar_fat(recipe)\n",
    "    data[i]['recipe']['sugar']=amt_sugar\n",
    "    data[i]['recipe']['fat']=amt_fat\n",
    "    data[i]['recipe']['dry']=amt_flour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop recipe if it doesn't contain sugar\n",
    "\n",
    "for i in range (0,len(data)-1):\n",
    "    if data[i]['recipe']['sugar']==0:\n",
    "        del data[i]\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data takes the form of a list of dictionaries\n",
    "# Write the list of dictionaries to a csv file\n",
    "\n",
    "keys=['social_rank','publisher','f2f_url','ingredients','source_url','recipe_id','image_url','publisher_url','title',\n",
    "     'sugar','fat','dry']\n",
    "import csv\n",
    "\n",
    "with open('all.csv', 'wb') as f:  \n",
    "    w = csv.DictWriter(f, keys, restval='')\n",
    "    w.writeheader()\n",
    "    for item in data:\n",
    "        try:\n",
    "            my_dict=item['recipe']\n",
    "            w.writerow(my_dict)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "# Add ratio columns\n",
    "\n",
    "df=pd.read_csv('/Users/ranaquadri/Documents/recipe/master1.csv')\n",
    "\n",
    "# Add ratio columns\n",
    "df['ratio_sugar_flour']=df['sugar']/df['dry']\n",
    "df['ratio_fat_flour']=df['fat']/df['dry']\n",
    "\n",
    "#print df.head()\n",
    "print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  ## Create binary variables for positive adj and health adj in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup1=['addictive', 'affair', 'amazing', 'amazingly', 'awesome', 'best', 'bodacious', 'bomb', 'bombshell', 'bursting'\n",
    "         , 'classic', 'decadence', 'decadent', 'delicious', 'die', 'easy', 'ever', 'excellent', 'fabulous', 'feel-good',\n",
    "         'heavenly', 'irresistible', 'luscious', 'perfect', 'ravishing', 'restaurant-quality', 'righteous', 'sex', \n",
    "         'simple', 'ultimate', 'ultimate', 'upscale', 'very', 'yummy', 'world-famous']\n",
    "\n",
    "lookup2=['calorie', 'fat-free', 'health', 'healthier', 'healthy', 'healthy-ish', 'gluten-free', 'lactose', 'light', \n",
    "         'lighter', 'low-carb', 'low-fat', 'makeover:', 'no-sugar-added', 'paleo', 'skinny', 'vegan', 'whole-grain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary column indicating whether positive adjectives are in the title\n",
    "import regex as re\n",
    "df['pos_adj']=0\n",
    "for i in range(0,len(lookup1)):\n",
    "    tt = lookup1[i] \n",
    "    for j in range(0,len(df)):\n",
    "        if re.search(tt,df['title'][j].lower()):\n",
    "            df['pos_adj'][j]=1                    \n",
    "#print df['pos_adj'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create binary column indicating whether health adjectives are in the title\n",
    "\n",
    "df['health_adj']=0\n",
    "for i in range(0,len(lookup2)):\n",
    "    tt = lookup2[i]   \n",
    "    for j in range(0,len(df)):\n",
    "        if re.search(tt,df['title'][j].lower()):\n",
    "            df['health_adj'][j]=1\n",
    "#print df['pos_adj'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text']=df[\"ingredients\"].map(str) + df[\"title\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df['text'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "n_components = 10\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, df, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print df[doc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingreds = df['text'].fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(ingreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = ingreds\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
